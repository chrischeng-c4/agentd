project_name = "agentd"
scripts_dir = "agentd/scripts"

# =============================================================================
# Workflow Iteration Settings
# =============================================================================
[workflow]
format_iterations = 3          # proposal format validation iterations
planning_iterations = 3        # proposal ↔ challenge iterations
implementation_iterations = 3  # implement ↔ review iterations
archive_iterations = 1         # archive review iterations

# =============================================================================
# Gemini Configuration
# =============================================================================
[gemini]
command = "gemini"
default = "flash"  # Default model ID

[[gemini.models]]
id = "flash"
model = "gemini-3-flash-preview"
complexity = "medium"  # Handles up to Medium complexity
cost_per_1m_input = 0.10   # USD per 1M input tokens
cost_per_1m_output = 0.40  # USD per 1M output tokens

[[gemini.models]]
id = "pro"
model = "gemini-3-pro-preview"
complexity = "critical"  # Handles up to Critical complexity
cost_per_1m_input = 1.25   # USD per 1M input tokens
cost_per_1m_output = 10.00 # USD per 1M output tokens

# =============================================================================
# Codex Configuration
# =============================================================================
[codex]
command = "codex"
default = "fast"  # Default model ID (gpt-5.2-codex low for testing)

[[codex.models]]
id = "fast"
model = "gpt-5.2-codex"
reasoning = "low"
complexity = "low"
cost_per_1m_input = 2.00   # USD per 1M input tokens
cost_per_1m_output = 8.00  # USD per 1M output tokens

[[codex.models]]
id = "balanced"
model = "gpt-5.2-codex"
reasoning = "medium"
complexity = "medium"
cost_per_1m_input = 2.00   # USD per 1M input tokens
cost_per_1m_output = 8.00  # USD per 1M output tokens

[[codex.models]]
id = "deep"
model = "gpt-5.2-codex"
reasoning = "high"
complexity = "high"
cost_per_1m_input = 2.00   # USD per 1M input tokens
cost_per_1m_output = 8.00  # USD per 1M output tokens

[[codex.models]]
id = "max"
model = "gpt-5.2-codex"
reasoning = "extra high"
complexity = "critical"
cost_per_1m_input = 2.00   # USD per 1M input tokens
cost_per_1m_output = 8.00  # USD per 1M output tokens

# =============================================================================
# Claude Configuration
# =============================================================================
[claude]
command = "claude"
default = "fast"  # Default model ID (haiku for testing)

[[claude.models]]
id = "fast"
model = "haiku"
complexity = "low"
cost_per_1m_input = 0.80   # USD per 1M input tokens
cost_per_1m_output = 4.00  # USD per 1M output tokens

[[claude.models]]
id = "balanced"
model = "sonnet"
complexity = "medium"
cost_per_1m_input = 3.00   # USD per 1M input tokens
cost_per_1m_output = 15.00 # USD per 1M output tokens

[[claude.models]]
id = "deep"
model = "opus"
complexity = "critical"
cost_per_1m_input = 15.00  # USD per 1M input tokens
cost_per_1m_output = 75.00 # USD per 1M output tokens

# =============================================================================
# Validation Rules
# =============================================================================
[validation]
required_headings = [
    "Specification:",
    "Overview",
    "Requirements",
]
requirement_pattern = '^R\d+:'
scenario_pattern = "Scenario:"
scenario_min_count = 1
require_when_then = true
when_pattern = '- \*\*WHEN\*\*'
then_pattern = '- \*\*THEN\*\*'

[validation.severity_map]
missing_heading = "High"
invalid_requirement_format = "High"
missing_scenario = "High"
missing_when_then = "High"
duplicate_requirement = "High"
broken_reference = "Medium"
